---
title: "SwiftKey Project_ Milestone Report"
author: "Zoey Le"
date: "September 6, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Large databases comprising of text in a target language are commonly used when generating language models for various purposes. In this project, we will explore the major features of the text data given for the Coursera Data Science Capstone through Johns Hopkins University. The project is sponsored by SwiftKey. The final purpose is to create text prediction application with R Shiny application that predicts words using a natural language processing model.There are 4 given language database, in particular, we will work with English database instead of the others which are Russian, German and Finnish.

The first step which is also the goal of this report is to do some basic overviews and some necessary cleanings in order to get familiar with the database as well as prepare for further prediction model creation. More specifically, we will remove from the data numbers, symbols, punctuation and other words that should not be predicted to increase prediction accuracy. Then we will observe some of the most frequently appear words including single, two and three word phrases. 


## Loading Data

```{r loading, message=FALSE}
library(tm)
library(wordcloud)
library(RWeka)
library(stringi)
library(stringr)
library(knitr)
library(kableExtra)
library(ggplot2)
#setwd("D:/test/Coursera/Capstone_final project")
blogs = readLines("final/en_US/en_US.blogs.txt", skipNul = T, encoding="UTF-8")
news = readLines("final/en_US/en_US.news.txt",skipNul = T, encoding="UTF-8")
twitter = readLines("final/en_US/en_US.twitter.txt",skipNul = T, encoding="UTF-8")

```


First of all, let's look at the data structures and overview.

```{r totaltable}

blog.size = round((file.info("final/en_US/en_US.blogs.txt")$size/1024^2),2)
new.size= round((file.info("final/en_US/en_US.news.txt")$size/1024^2),2)
twitter.size = round((file.info("final/en_US/en_US.twitter.txt")$size/1024^2),2)

sum.tab=data.frame(file=c("Blogs","News","Twitter"),
                   size=c(blog.size,new.size,twitter.size),
                   lines=c(length(blogs),length(news),length(twitter)),
                   words=c(sum(stri_count_words(blogs)),
                           sum(stri_count_words(news)),
                           sum(stri_count_words(twitter))))           
names(sum.tab)=c("File","Size(Mb)","Number of Lines","Number of Words")

kable(sum.tab) %>%
  kable_styling(bootstrap_options="striped",full_width=F)

```

Since the volume of the given database is really big, 1000 of lines will be used for the demonstration of Cleaning and Exploratory Analysis in this Milestone report. Below is the summary table of sample data. 

```{r sampletable}
set.seed(324)
sample.size=1000

sample.blog=sample(blogs,sample.size)
sample.new=sample(news,sample.size)
sample.twitter=sample(twitter,sample.size)

sum.samtab=data.frame(file=c("Sample Blogs","Sample News","Sample Twitter"),
                      size=round((sample.size/c(length(blogs),length(news),length(twitter)))
                                 *c(blog.size,new.size,twitter.size),2),
                      lines=c(length(sample.blog),length(sample.new),length(sample.twitter)),
                      words=c(sum(stri_count_words(sample.blog)),
                              sum(stri_count_words(sample.new)),
                              sum(stri_count_words(sample.twitter))))
names(sum.samtab)=names(sum.tab)

kable(sum.samtab) %>%
  kable_styling(bootstrap_options="striped",full_width=F)

```

## Cleaning data

For cleaning the text data for further text mining process, we will follow below steps:

- Create Corpus: corpus is created to use in tm_map for text cleaning
- Remove URLs and website addresses by change internet addresses into blank space
- Lowercase all words
- Remove numbers
- Remove stopwords by replace them into blank space
- Remove profinity using bad-words.txt from http://www.bannedwordlist.com/lists/swearWords.txt
- Remove punctuations
- Remove strip white space and create plain text documents

```{r cleaning}
#####CLEANING

sample=c(sample.blog,sample.new,sample.twitter)

##lower case
sample=tolower(sample)

##remove website link and twitter @
sample=gsub("(f|ht)tp(s?)://(.*)[.][a-z]+", " ", sample)
sample=gsub("@[^\\s]+"," ",sample)

##remove latin1 words
latin.sym=grep("[^NOT_ASCII](NOT_ASCII){2}[^NOT_ASCII]",iconv(sample, "latin1", "ASCII", sub="NOT_ASCII"))
sample[latin.sym]=stri_trans_general(sample[latin.sym], "latin-ascii")
sample=str_replace_all(sample, "[[:punct:]]", "'")
sample=gsub('[^\x20-\x7E]', "'", sample)

##remove stopwords
sample=removeWords(sample,stopwords("en"))

##remove punctuations
sample=removePunctuation(sample)

##remove numbers
sample=removeNumbers(sample)

##remove profinity
swear.words = read.table(file ="swearWords.txt", stringsAsFactors=F)
sample=removeWords(sample,swear.words[,1])

##remove extra space
sample=stripWhitespace(sample)

##create corpus
corpus = VCorpus(VectorSource(sample))
corpus = tm_map(corpus, PlainTextDocument)

```

## Frequency Table and Visualization by WordCloud

We will examine top 30 most frequently word combination appearance. In particular, unigram, digram and trigram word combination.The frequency will be shown as table, histogram and wordcloud.

```{r freq}

top=30 #number of top frequent appear words

#Frequency table
getFreq = function(tdm,ngram) {
  gram=function(x) NGramTokenizer(x,Weka_control(min=ngram,max=ngram))
  tdm = TermDocumentMatrix(tdm,control= list(tokenizer=gram))
  freq1 = sort(rowSums(as.matrix(tdm)), decreasing = TRUE)
  freq=data.frame(word = names(freq1), freq = freq1)
  freq$word=as.character(freq$word)
  return(freq)
}

#Frequency Histogram
makePlot = function(table, label) {
  ggplot(table, aes(reorder(word, freq), freq)) +
    geom_bar(stat = "identity", fill = I("royalblue3"))+
    coord_flip()+
    labs(x = label, y = "Frequency")
}


```

###1. Unigram

####a. Frequency Table

```{r table1}

uni.freq = getFreq(corpus,1)

kable(uni.freq[1:top,],row.names = F) %>%
  kable_styling(bootstrap_options="striped",full_width=F)%>%
  scroll_box(width = "100%", height = "400px")

```

####b. Frequency Histogram

```{r hist1}

makePlot(uni.freq[1:top,], "30 Most Common Unigrams")

```

####c. Frequency Wordcloud

```{r wordcloud1}

wordcloud(words = uni.freq$word, freq = uni.freq$freq, min.freq = 1,
            max.words=50, random.order=T, rot.per=0.35, random.color = F,scale=c(4,.2),
            colors=brewer.pal(8,name= "Set1"))


```

###2. Digram

####a. Frequency Table


```{r table2}

bi.freq = getFreq(corpus,2)

kable(bi.freq[1:top,],row.names = F) %>%
  kable_styling(bootstrap_options="striped",full_width=F)%>%
  scroll_box(width = "100%", height = "400px")

```


####b. Frequency Histogram


```{r hist2}

makePlot(bi.freq[1:top,], "30 Most Common Digrams")

```


####c. Frequency Wordcloud


```{r wordcloud2}

wordcloud(words = bi.freq$word, freq = bi.freq$freq, min.freq = 1,
            max.words=30, random.order=T, rot.per=0.35, random.color = F,scale=c(3,.1),
            colors=brewer.pal(8,name= "Set1"))


```



###3. Trigram


####a. Frequency Table

```{r table3}

tri.freq = getFreq(corpus,3)

kable(tri.freq[1:top,],row.names = F) %>%
  kable_styling(bootstrap_options="striped",full_width=F)%>%
  scroll_box(width = "100%", height = "400px")

```


####b. Frequency Histogram


```{r hist3}

makePlot(tri.freq[1:top,], "30 Most Common Trigrams")

```


####c. Frequency Wordcloud


```{r wordcloud3, fig.height=6,fig.width=6}


  wordcloud(words = tri.freq$word, freq = tri.freq$freq, min.freq = 1,
            max.words=30, random.order=T, rot.per=0.35, random.color = T,scale=c(2,.1),
            colors=brewer.pal(8,name= "Set1"))


```

## Plans for prediction process and Shiny app

We now get familiar to and have an overview look at the database. Hence, for the next step, we will move to building prediction model based on the frequency of common word combination we had create above.

Finally, we will apply the prediction algorithm to create a Shiny app as well as a R presentation to show off and testing the result. 


